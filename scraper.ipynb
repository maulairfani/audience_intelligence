{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "api_key = \"AIzaSyCI0cHHN4dD1D5VVmqIM-igV-Lq0Rpt2j0\"\n",
    "youtube = build('youtube', 'v3',developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terdapat 30 video pada playlist\n"
     ]
    }
   ],
   "source": [
    "# Ambil semua judul dalam playlist\n",
    "request = youtube.playlistItems().list(\n",
    "    part=\"snippet\",\n",
    "    playlistId=\"PLkiSnq8pdz9T3QQVbczz4XVgsHjjJK3vd\", # Playlist id bocor alus,\n",
    "    maxResults=50\n",
    ")\n",
    "playlist = request.execute()\n",
    "playlist_items = playlist[\"items\"]\n",
    "\n",
    "# Ambil video id\n",
    "video_ids = list()\n",
    "for item in playlist_items:\n",
    "    if item[\"snippet\"][\"title\"] != \"Deleted video\":\n",
    "        video_ids.append(item[\"snippet\"][\"resourceId\"][\"videoId\"])\n",
    "\n",
    "print(f\"Terdapat {len(video_ids)} video pada playlist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:01<00:00, 16.80it/s]\n"
     ]
    }
   ],
   "source": [
    "data = dict()\n",
    "for i in tqdm(range(len(video_ids))):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"contentDetails,id,liveStreamingDetails,localizations,player,recordingDetails,snippet,statistics,status,topicDetails\",\n",
    "        id=video_ids[i]\n",
    "    )\n",
    "    response = request.execute()\n",
    "    data[video_ids[i]] = response\n",
    "\n",
    "with open(\"data.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SQXcaHsG0Fc', 'mBPb_ZWSsSU', 'iyhepYA0qU8', 'Yxxap3815IM', '-7-DT39jnas', 'sUdAvJ0lUHQ', 'z30vDjd1s3U', 'DhjzluWJXKY', 'CUpC_08jbUU', 'oovSMZlzA3o', 'jlmBvgvhBfQ', '5AMf9MqnYNE', 'Nv56vD2XLbw', 'aV0YFRu3sVQ', 'uqoVC5BISlA', 'Vvl_Nd_S-Ng', 'Toz5-ZFVO9s', 'oChmn56mgOc', 'xOqt_WNOhxQ', 'CcB6QnYPR3s', 'ED4tTy8bDGk', '-u4THO6xOt0', 'w-lphu4KdcY', 'mT42kKwceeU', 'xs6AL3xfpFU', 'ufnRKFihJ9M', 'xTSABfhXFFI', 'D_LmGMCnih0', 'Xjekfmg70wI', 'zKVC0y0CoGw', 'YOJ9L4M9Mgo'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data.json\") as file:\n",
    "    d = json.load(file)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langsung scraping setiap video yang ada di playlist, outputnya jadi satu csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllTopLevelCommentReplies(topCommentId, token): \n",
    "  \"\"\"\n",
    "  Recursive function that retrieves the replies a given comment has.\n",
    "  \"\"\"\n",
    "\n",
    "  replies_response=youtube.comments().list(part='snippet',maxResults=100,parentId=topCommentId,pageToken=token).execute()\n",
    "\n",
    "  for indx, reply in enumerate(replies_response['items']):\n",
    "    all_comments.append(reply['snippet']['textDisplay'])\n",
    "    \n",
    "  if \"nextPageToken\" in replies_response: \n",
    "    return getAllTopLevelCommentReplies(topCommentId, replies_response['nextPageToken'])\n",
    "  else:\n",
    "    return []\n",
    "      \n",
    "def get_comments(youtube, video_id, token): \n",
    "  \"\"\"\n",
    "  Recursive function that retrieves the comments (top-level ones) a given video has.\n",
    "  \"\"\"\n",
    "\n",
    "  global all_comments\n",
    "  global all_authors\n",
    "  totalReplyCount = 0\n",
    "  token_reply = None\n",
    "\n",
    "  if (len(token.strip()) == 0): \n",
    "    all_comments = []\n",
    "\n",
    "  if (token == ''): \n",
    "    video_response=youtube.commentThreads().list(part='snippet',maxResults=100,videoId=video_id,order='relevance').execute() \n",
    "  else: \n",
    "    video_response=youtube.commentThreads().list(part='snippet',maxResults=100,videoId=video_id,order='relevance',pageToken=token).execute() \n",
    "\n",
    "  # Loop comments from the video: \n",
    "  for indx, item in enumerate(video_response['items']): \n",
    "    # Append coments:\n",
    "    all_comments.append(\"COMMENT WITH \" + str(item['snippet']['totalReplyCount']) + \" replies: \" + item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "    all_authors.append(item['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
    "    # Get total reply count: \n",
    "    totalReplyCount = item['snippet']['totalReplyCount']\n",
    "\n",
    "    # If the comment has replies, get them:\n",
    "    if (totalReplyCount > 0): \n",
    "      # Get replies - first batch: \n",
    "      replies_response=youtube.comments().list(part='snippet',maxResults=100,parentId=item['id']).execute()\n",
    "      for indx, reply in enumerate(replies_response['items']):\n",
    "        # Append the replies to the main array: \n",
    "        all_comments.append((\" \"*2) + \"=>FIRST CALLBACK REPLY: \" + reply['snippet']['textDisplay'])\n",
    "        all_authors.append(reply['snippet']['authorDisplayName'])\n",
    "\n",
    "      # If the reply has a token for get more replies, loop those replies \n",
    "      # and add those replies to the main array: \n",
    "      while \"nextPageToken\" in replies_response:\n",
    "        token_reply = replies_response['nextPageToken']\n",
    "        replies_response=youtube.comments().list(part='snippet',maxResults=100,parentId=item['id'],pageToken=token_reply).execute()\n",
    "        for indx, reply in enumerate(replies_response['items']):\n",
    "          all_comments.append((\" \"*4) + \"==>WHILE GETTING REPLIES: \" + reply['snippet']['textDisplay'])\n",
    "          all_authors.append(reply['snippet']['authorDisplayName'])\n",
    "    \n",
    "  # Check if the video_response has more comments:\n",
    "  if \"nextPageToken\" in video_response: \n",
    "    return get_comments(youtube, video_id, video_response['nextPageToken']) \n",
    "  else: \n",
    "    # Remove empty elements added to the list \"due to the return in both functions\":\n",
    "    all_comments = [x for x in all_comments if len(x) > 0]\n",
    "    # print(\"Fin\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [03:31<00:00, 17.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Proses scraping\n",
    "df_comments = pd.DataFrame(columns=[\"video_id\", \"author\", \"comment\"])\n",
    "for i in tqdm(range(len(video_ids))):\n",
    "    all_comments=[]\n",
    "    all_authors=[]\n",
    "    qtyReplies = 0\n",
    "    qtyMainComments = 0\n",
    "\n",
    "    result = get_comments(youtube, video_ids[i], '')\n",
    "\n",
    "    temp = pd.DataFrame({\n",
    "        \"video_id\": [video_ids[i]] * len(all_comments),\n",
    "        \"comment\": all_comments,\n",
    "        \"author\": all_authors\n",
    "    })\n",
    "    df_comments = df_comments.append(temp, ignore_index=True)\n",
    "\n",
    "    df_comments.to_csv(\"all_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import undetected_chromedriver as uc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jadi ada seorang kepala daerah ya kan di salah satu provinsiah di Pulau Jawa gitu ya itu cerita dia \n",
      "1 Jauh sebelum Gibran diumumkan menjadi cawapres tahun ini berarti ya tahun ini betul Bu Iriana itu bi\n",
      "2 menurut E ceritanya jimli adalah lah kalau kemudian eh Anwar Usman dipecat sebagai Hakim anggota tid\n",
      "3 dan kovifa ini sendiri ee mendapatkan ee permintaannya atau mendapatkan ee ee mendengar permintaan u\n",
      "4 lu yang yang memainkan dinasti politik Terus lu kemudian menyerahkan pada rakyat gitu dan ini pasti \n",
      "5 Anwar Usman ini mendatangi beberapa hakim MK dan EE mengatakan kalau bisa kita e cepat saja mengelua\n",
      "6 pertanyaan gue Apakah Jokowi mendukung Gibran menjadi capresnya Prabowo apa INF yang gu dapat menari\n",
      "7 ibu sebagai caes saya gitu kan Nah ka balik bertanya ini Pak Jokowi Pak Lur ke mana gitu kan gitu na\n",
      "8 prabjar karena e di sini e apa namanya yang yang terakhir manuver-manuver atau lobi untuk mewujudkan\n",
      "9 Puan Maharani dengan Ahya juga komunikasi kan siapa yang mengirimkan pesan nih tapi bocoran yang gua\n",
      "10 sebentar sebentar gua harus gulung dulu kan pengen duduk setelah bertemu dengan Megawati Ridwan Kami\n",
      "11 tapi ada yang bilang begini mas guru spiritual emas sama ibunda Mas itu menyebutnya Mas Ahai sebagai\n",
      "12 kok ada Tudingan memaksakan Ayah sebagai capres jangan-jangan bagi Pak Anis Yang pentingnya press da\n",
      "13 kalau orang dekatnya SBY yang hadir di situ bilangnya pak SB ini kemarahannya sudah di ubun-ubun [Te\n",
      "14 masih ada nanya apa aja gua jawab rahasia-rasa negara semua di sini belum nanya apa aja gua jawab ap\n",
      "15 \n",
      "15 ini biar on cam biar on cam Selamat datang Selamat datang Selamat datang itu dari Unair ya kan Yang \n",
      "16 ini perempuan Indonesia memikirkan tentang arsitektur ruang angkasa planet sementara presidennya di \n",
      "17 [Musik] Jumpa lagi bersama saya Stefan supramono di bocoran politik kali ini kami memberikan hak jaw\n",
      "18 sebenarnya terjadi di dalam PDIP itu sendiri seperti apa coba bocorkan nah ini menarik ya kalau mau \n",
      "19 Apakah Nanti Cak Imin yang akan maju sendiri Sin RAM atau dia menyerahkan kursi itu ke calon lain gi\n",
      "20 tapi seberapa kuat sih Shin pengaruh Luhut di Golkar itu sehingga dia bisa jadi seberapa seriusnya p\n",
      "21 bernama koalisi perubahan itu ya salah total Kenapa bos sandi dari Gerindra ke P3 itu termasuk permi\n",
      "22 cerita yang kita dapat dibaliknya adalah ada faktor ketidaksiapan infrastruktur berarti ketika Erik \n",
      "23 Prabowo juga sudah meminta bantuan dari peruju Kenapa pemenangan loh bukan mau ini kan kita ngerasa \n",
      "24 di situ Prabowo menawarkan lagi lah mengajak Khofifah untuk Gimana jadi wakil saya gitu nah Khofifah\n",
      "25 ini kan kejahatan rezim ada sistem ya atau pola Bang dalam sistem itu Lalu ada pola yang berjalan ka\n",
      "26 Anis batal atau gagal maju sebagai kontestan sudah menjajaki sana-sini gitu kan tokoh yang potensial\n",
      "27 mobil baru gue dikit Loh oke mobil baru gue cuman apa sih dia dua Abangnya mobil tua semua udah hart\n",
      "28 Ah ini Aduh Awas ada di rekornya direktur kamera kamera itu kan Ganjar kayak disekap tapi itu yang d\n",
      "29 Halo selamat siang ini kita tapping siang-siang hari di program baru tempo namanya bocor alus apa ki\n"
     ]
    }
   ],
   "source": [
    "captions = dict()\n",
    "driver = uc.Chrome()\n",
    "driver.get(\"https://anthiago.com/transkrip/\")\n",
    "for i in range(len(video_ids)):\n",
    "    num_retry = 0\n",
    "    text = \"\"\n",
    "    while len(text) < 100 and num_retry < 3:\n",
    "        time.sleep(3)\n",
    "        url = f\"www.youtube.com/watch?v={video_ids[i]}\"\n",
    "        driver.find_element(By.CSS_SELECTOR, \"#url_input\").send_keys(url)\n",
    "        driver.find_element(By.CSS_SELECTOR, \"#url_input\").send_keys(Keys.ENTER)\n",
    "        time.sleep(10)\n",
    "        c = driver.find_element(By.CSS_SELECTOR, \"#result\")\n",
    "        text += c.text\n",
    "        driver.refresh()\n",
    "        captions[video_ids[i]] = text\n",
    "        print(i, captions[video_ids[i]][:100])\n",
    "        \n",
    "        num_retry += 1\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "with open(\"captions.json\", \"w\") as outfile:\n",
    "    json.dump(captions, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
